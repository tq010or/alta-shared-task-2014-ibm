\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath,relsize}
\usepackage{pifont}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\dataset}[1]{\texttt{#1}\xspace}
\newcommand{\mygl}[1]{``#1''}
\newcommand{\mybf}[1]{\textbf{#1}}
\newcommand{\myex}[1]{\textit{#1}}
\newcommand{\lexpair}[2]{\myex{#1}\xspace\mygl{#2}}
\newcommand{\method}[1]{\textsf{#1}\xspace}
\newcommand{\x}{\phantom{0}}
\newcommand{\ngram}{\textit{n}-gram\ }
\newcommand{\stanford}{\textsc{Stanford}\xspace}
\newcommand{\washington}{\textsc{Washington}\xspace}
\newcommand{\msra}{\textsc{msra}\xspace}
\newcommand{\cmu}{\textsc{cmu}\xspace}
\newcommand{\alta}{\textsc{ALTA}\xspace}
\newcommand{\feature}[1]{\texttt{#1}\xspace}
\newcommand{\myurl}[1]{{\footnotesize\url{#1}}}

\newcommand{\secref}[2][]{Section#1 \ref{#2}}
\newcommand{\secrefs}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\tabref}[2][]{Table#1 \ref{#2}}
\newcommand{\tabrefs}[2]{Tables \ref{#1} and \ref{#2}}
\newcommand{\figref}[2][]{Figure#1 \ref{#2}}
\newcommand{\eqnref}[2][]{Equation#1 \ref{#2}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\title{Identifying Twitter Location Mentions}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    This paper describes a participant system in ALTA shared task 2014.
    The task is to identify location mentions in Twitter messages, such as place names and point-of-interests (POIs).
    We formulated the task as a sequential labelling problem, and explored various features on top of a conditional random field (CRF) classifier.
    The system achieved 0.712 mean-F measure on the held-out evaluation data.
    We discussed our results and provided future work on named entity recognition in social media.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The ALTA shared task 2014 aims to identify place mentions in Twitter data.
The input is plain text messages, and the expected output is location entities such as country names, city names and POIs for each corresponding messages.
For instance, \myex{Auckland} and \myex{\#eqnz} are identified as location mentions in \myex{@USER are you considering an Auckland visit after \#eqnz today?}.\footnote{\myex{\#eqnz} is short for earthquake in New Zealand.}
This shared task is very much similar to a well-established NLP task --- named entity recognition (NER) with a focus on location entities.
For each token in a text message, it is categorised as either a location mention or not.
The nearby words (\ie context) may influence a word's labelling, hence we incorporate context information in our system.
Following the literature on NER \cite{wwwc13ling}, we formulate it as a sequential labelling task and use a conditional random field (CRF) as the classifier.

The main contributions of the paper are:
(1) A sequential labeller for identifying location mentions in social media;
(2) Feature analysis and comparison in NER between social media and other genres.
(3) Additional experiments and discussion on extending current sequential labeller.

\section{Challenges}
\label{sec:challenge}
Although CRF models for NER are widely used and are reported to achieve state-of-the-art results in literature \cite{acl05fink,acl11liux,emnlp11ritt}, NER in social media still raises several non-trivial challenges.

First, social media text is noisy.
It contains many non-standard words including typos (\eg \lexpair{challanges}{challenges}), abbreviations (\eg \lexpair{ppl}{people}) and phonetic substitutions (\eg \lexpair{4eva}{forever}).
These non-standard words often cause generalisation issue \cite{acl11han}.
For instance, lexical variants (\eg \myex{Melb}, \myex{Mel}, \myex{melbn}) won't be recognised in the test data when only standard forms (\eg \mygl{Melbourne}) are observed in the training data.

In addition to non-standard words, informal and colloquial writing style further challenges the NER accuracy.
One example is conventional features relying on capitalisation are less reliable in social media.
For instance, \myex{LOL} is capitalised but it is not a location entity.
By contrast, \myex{brisbane} is a valid location mention even though it is in lowercase.

Similarly, Twitter specific entities sometimes are sentence constituents, \eg \myex{\#Melbourne} in \myex{\#Melbourne is my fav city.}
However, they may be a topic tag that is non-compliant to the sentence syntactic structure, \eg hashtags in \myex{I like travel to beautiful places, \#travel \#melbourne}.
For the latter case, syntactic features would be less effective.

To this end, widely-used NER features require to be re-examined in social media.

\section{Feature Engineering}
\label{sec:feature}

\subsection{Brief Litearture on NER Feature Enginnering}
\label{sec:literature}
We engineer features by reviewing some representative systems.
The main features of representative systems are summarised in \tabref{tab:fea_comp}.

\stanford NER combined Gibbs sampling and a widely used CRF model \cite{acl05fink}.
The Gibbs sampling offers non-local constraints to the conventional CRF model that utilises a range of local features.
The features in the CRF model include word features, POS tag features, character \ngram features, word shape features and the presence of words in a pre-defined window.
The word and POS tag features also includes the surrounding tokens and tags to capture the local context information.

\newcite{acl11liux} proposed a two-stage CRF-based tagger \msra for Twitter NER.
First, a \textit{k}-NN classifier pre-categorises words, and then feeds results to a downstream CRF modeller.
The features they adopted in \textit{k}-NN are two word text windows including the target word (\ie five words in total).
The gazetted resources (from Wikipedia) are also utilised and shown effective in their experiments.
As for the features for building the second stage CRF model, they follow \newcite{conll09rati} and made use of words tokens, word types (\eg whether the word is alphanumeric or capitalised?), word morphological features (\eg suffix and prefix of words), previous tagging labels, word context windows, and conjunction features that combine both tags and word context windows.
%Among various extensions on their experiments, they also found gazetteers are quite helpful on both social media and existing datasets.
%Clustering lexical variants seems a complex story, it doesn't show clear improvement on social media dataset (as reported), but bears much improvements on the conventional datasets.

Recently, another \washington NER tool was developed by rebuilding a Twitter-specific NLP pipeline (\eg from tokenisation and POS tagging to chunking and NER).
They adopted rich information generated in the pipeline, such as POS tags, chunking and predicted capitalisation information, as well as Brown clustering of lexical variants and gazetted features from Freebase.

% table 
\begin{table*}[!htp]
\begin{center}
\begin{tabular}{lccc}
\hline 
Features                                     & \stanford & \msra & \washington \\ 
\hline
\feature{Word}                               & \cmark & \cmark & \cmark \\
\feature{Word Context}                       & \cmark & \cmark & \cmark \\
\feature{Word Morphology}                    & Character \ngram  & Affix & Brown Cluster \\
%\feature{Non-local constraints}              & Gibbs sampling & \textit{k}-NN results & labelled-LDA \\
\feature{POS}                                & \cmark & \xmark & in-domain POS tagger\\
\feature{Chunking}                           & \xmark & \xmark & in-domain chunker \\
\feature{Capitalisation}                     & \xmark & \cmark & in-domain capitalisation restoration\\
\feature{Gazetteers}                         & \xmark & Wikipedia & Freebase \\
\hline
\end{tabular}
\end{center}
\caption{Features comparison of represtative NER Systems}
\label{tab:fea_comp}
\end{table*}


\subsection{Proposed Features}
\label{sec:basic_feature}

Based on the previous representative NER work, we select our features with justifications as follows:
\begin{itemize}
    \item \feature{Word}. Lowercased word types are included as a default feature as suggested by existing systems. Previous and next words are also included to capture local context information. Larger context window size is not considered as Twitter data is quite terse and ungrammatical, incorporating long distance context may bring little context information but introduce more noise;
    \item \feature{POS}. Based on the fact that location named entities are primarily nouns. A reliable POS tagger generates valuable clues for locations. Instead of re-building an NLP pipeline, we adopt an off-the-shelf Twitter POS tagger \cmu that generates coursed-grained POS tags with high accuracy ($\ge$ 90\%) \cite{naacl13owop}. Similar to \feature{Word}, the previous and next \feature{POS} tags are also included.
    \item \feature{Capitalisation}. Instead of normalising token case in Twitter (\eg \cite{emnlp11ritt}), four types of capitalisation information are retrieved based on the original surface form. Namely, they are all character uppercased (AU), all character lowercased (AL), first character uppercased and the rest are lowercased (UL) and mixed capitalisation (MC). 
    \item \feature{Domain}. 
    \item \feature{Gazetteer}. 
    \item \feature{Deep Learning}.
\end{itemize}

hashtag and user mention

\subsection{Gazetted Features}

GeoNames\footnote{GeoNames site: \url{http://www.geonames.org}} is a geographical database with information about all countries with over eight million place names, such as city names and points of interest (POI).

Some place names in GeoNames are used as well commonly without denoting a location.
Examples of these terms include person names, natural disasters (e.g. storm), and names that usually do not denote a location (e.g. Friday or Friend).
We collected stopwords starting with a standard one and we added the 5 thousand most frequent English terms\footnote{http://www.wordfrequency.info}, natural disaster names from Wikipedia and a list of popular person names\footnote{https://online.justice.vic.gov.au/bdm/popular-names}.

After extracting and cleaning the terms from GeoNames, the list has over 9.8 million terms.
The dictionary has been used to annotate the tweets using ConceptMapper~\cite{tanenblatt2010conceptmapper}.
The GeoNames annotation has been to indicate to the CRF which tokens might denote a location.

\subsection{Deep Features}

Small data amount

Deep learning
Semi-supervised learning



\section{Experiments and Discussion}
\label{sec:experiment}

Experiment number with feature ablations

\section{Discussion}
\label{sec:discussion}
%% We could talk about the issues with our approach

Our system incorrectly identified some tokens as locations.
Most of the false positives were due to CRF mistakes.
Examples of these mistakes are annotation of tokens like \textit{bushfires}, 
Probably a larger data set would allow the CRF model to avoid these mistakes.
On the other hand, many false positives produced by our system look as genuine locations.
For instance, \textit{bakery} was not annotated in \textit{Chinatown bakery} but is was annotated in \textit{Manchester Wong Wong's bakery} a few tweets below.
Some locations such as \textit{Kumbarilla State Forest} or \textit{Shire of Carnar} seem to be false positives as well.
Possibly the noise in the data set is responsible as well for errors produced by our CRF tagger.

Even with our best efforts to remove location names that would not typically denote a location, there are some GeoNames locations in our dictionary that typically do not denote a location, e.g.~\textit{the end of the world}.

Our system missed some Twitter user names or hashtags with location information (e.g. \textit{@FireRescueNSW}, \textit{@abcsouthqld}, \textit{\#NZquake})
These locations contain an acronym or abbreviation denoting a location as prefix or suffix of a word that was not in our dictionary.
For some locations, the acronym or abbreviation denoting a location was not in our location dictionary (e.g. \textit{Melb}).

Some location names were not in our GeoName dictionary and were neither identified by the CRF.
Examples of these location names include \textit{Coal Quay}, \textit{Pretty Bach} or \textit{Massabielle grotto}.
Some two letter US states acronyms were not recognized by our system, e.g. OR, IR, MT or MT.

In a few cases, our system missed part of the location name when it was a specific location if a location name.
For instance, \textit{markets} was not annotated in \textit{Kelvin Grove markets} or \textit{grounds} was not annotated in \textit{UTS grounds}.


\section{Conclusion and Future Work}
\label{sec:conclusion}


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{strings,acl2014}

\end{document}
