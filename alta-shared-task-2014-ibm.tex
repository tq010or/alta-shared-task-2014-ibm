\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath,relsize}
\usepackage{pifont}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\dataset}[1]{\texttt{#1}\xspace}
\newcommand{\mygl}[1]{``#1''}
\newcommand{\mybf}[1]{\textbf{#1}}
\newcommand{\myex}[1]{\textit{#1}}
\newcommand{\lexpair}[2]{\myex{#1}\xspace\mygl{#2}}
\newcommand{\classname}[1]{\textsf{#1}\xspace}
\newcommand{\x}{\phantom{0}}
\newcommand{\ngram}{\textit{n}-gram\ }
\newcommand{\stanford}{\textsc{Stanford}\xspace}
\newcommand{\washington}{\textsc{Washington}\xspace}
\newcommand{\alta}{\textsc{ALTA}\xspace}
\newcommand{\feature}[1]{\texttt{#1}\xspace}
\newcommand{\myurl}[1]{{\footnotesize\url{#1}}}

\newcommand{\secref}[2][]{Section#1 \ref{#2}}
\newcommand{\secrefs}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\tabref}[2][]{Table#1 \ref{#2}}
\newcommand{\tabrefs}[2]{Tables \ref{#1} and \ref{#2}}
\newcommand{\figref}[2][]{Figure#1 \ref{#2}}
\newcommand{\eqnref}[2][]{Equation#1 \ref{#2}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\title{Identifying Twitter Location Mentions}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    This paper describes a participant system in ALTA shared task 2014.
    The task is to identify location mentions in Twitter messages, such as place names and point-of-interests (POIs).
    We formulated the task as a sequential labelling problem, and explored various features on top of a conditional random field (CRF) classifier.
    The system achieved 0.712 mean-F measure on the held-out evaluation data.
    We discussed our results and provided future work on named entity recognition in social media.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The ALTA shared task 2014 aims to identify place mentions in Twitter data.
The input is plain text messages, and the expected output is location entities such as country names, city names and POIs for each corresponding messages.
For instance, \myex{Auckland} and \myex{\#eqnz} are identified as location mentions in \myex{@USER are you considering an Auckland visit after \#eqnz today?}.\footnote{\myex{\#eqnz} is short for earthquake in New Zealand.}
This shared task is very much similar to a well-established NLP task --- named entity recognition (NER) with a focus on location entities.
For each token in a text message, it is categorised as either a location mention or not.
The nearby words (\ie context) may influence a word's labelling, hence we incorporate context information in our system.
Following the literature on NER \cite{wwwc13ling}, we formulate it as a sequential labelling task and use a conditional random field (CRF) as the classifier.

The main contributions of the paper are:
(1) A sequential labeller for identifying location mentions in social media;
(2) Feature analysis and comparison in NER between social media and other genres.
(3) Additional experiments and discussion on extending current sequential labeller.

\section{Challenges}
\label{sec:challenge}
Although CRF models for NER are widely used and are reported to achieve state-of-the-art results in literature \cite{acl05fink,acl11liux,emnlp11ritt}, NER in social media still raises several non-trivial challenges.

First, social media text is noisy.
It contains many non-standard words including typos (\eg \lexpair{challanges}{challenges}), abbreviations (\eg \lexpair{ppl}{people}) and phonetic substitutions (\eg \lexpair{4eva}{forever}).
These non-standard words often cause generalisation issue \cite{acl11han}.
For instance, lexical variants (\eg \myex{Melb}, \myex{Mel}, \myex{melbn}) won't be recognised in the test data when only standard forms (\eg \mygl{Melbourne}) are observed in the training data.

In addition to non-standard words, informal and colloquial writing style further challenges the NER accuracy.
One example is conventional features relying on capitalisation are less reliable in social media.
For instance, \myex{LOL} is capitalised but it is not a location entity.
By contrast, \myex{brisbane} is a valid location mention even though it is in lowercase.

Similarly, Twitter specific entities sometimes are sentence constituents, \eg \myex{\#Melbourne} in \myex{\#Melbourne is my fav city.}
However, they may be a topic tag that is non-compliant to the sentence syntactic structure, \eg hashtags in \myex{I like travel to beautiful places, \#travel \#melbourne}.
For the latter case, syntactic features would be less effective.

To this end, widely-used NER features requires to be re-examined in social media.

\section{Feature Engineering}
\label{sec:feature}
% stanford + EMNLP 2011 (Alan) system
We engineer features on basis of existing successful systems.
The widely used \stanford and Twitter-specific \washington are chosen for comparison.
Main features are highlighted in \tabref{tab:fea_comp}.

% table 
\begin{table}[h]
\begin{center}
\begin{tabular}{lccc}
\hline 
Features & \stanford & \washington & \alta \\ 
\hline
\feature{BoW} & \cmark & & \\
\feature{BoW-2} & & & \\
\feature{POS} & & & \\
\feature{POS-2} & & & \\
\hline
\end{tabular}
\end{center}
\caption{Referred features in literature}
\label{tab:fea_comp}
\end{table}

Basic features such as bag-of-words and part-of-speech tags are included.

\subsection{Domain-specific Features}
hashtag and user mention

\subsection{Gazetted Features}

GeoNames\footnote{GeoNames site: \url{http://www.geonames.org}} is a geographical database with information about all countries with over eight million place names, such as city names and points of interest (POI).

Some place names in GeoNames are used as well commonly without denoting a location.
Examples of these terms include person names, natural disasters (e.g. storm), and names that usually do not denote a location (e.g. Friday or Friend).
We collected stopwords starting with a standard one and we added the 5 thousand most frequent English terms\footnote{http://www.wordfrequency.info}, natural disaster names from Wikipedia and a list of popular person names\footnote{https://online.justice.vic.gov.au/bdm/popular-names}.

After extracting and cleaning the terms from GeoNames, the list has over 9.8 million terms.
The dictionary has been used to annotate the tweets using ConceptMapper~\cite{tanenblatt2010conceptmapper}.
The GeoNames annotation has been to indicate to the CRF which tokens might denote a location.

\subsection{Deep Features}

Small data amount

Deep learning
Semi-supervised learning



\section{Experiments and Discussion}
\label{sec:experiment}

\subsection{CRF Model}
\subsection{CRF Model + Deep Features}
\subsection{CRF Model + Semi-supervised Learning}


Experiment number with feature ablations

\section{Discussion}
\label{sec:discussion}
%% We could talk about the issues with our approach

Our system incorrectly identified some tokens as locations.
Most of the false positives were due to CRF mistakes.
Examples of these mistakes are annotation of tokens like \textit{bushfires}, 
Probably a larger data set would allow the CRF model to avoid these mistakes.
On the other hand, many false positives produced by our system look as genuine locations.
For instance, \textit{bakery} was not annotated in \textit{Chinatown bakery} but is was annotated in \textit{Manchester Wong Wong's bakery} a few tweets below.
Some locations such as \textit{Kumbarilla State Forest} or \textit{Shire of Carnar} seem to be false positives as well.
Possibly the noise in the data set is responsible as well for errors produced by our CRF tagger.

Even with our best efforts to remove location names that would not typically denote a location, there are some GeoNames locations in our dictionary that typically do not denote a location, e.g.~\textit{the end of the world}.

Our system missed some Twitter user names or hashtags with location information (e.g. \textit{@FireRescueNSW}, \textit{@abcsouthqld}, \textit{\#NZquake})
These locations contain an acronym or abbreviation denoting a location as prefix or suffix of a word that was not in our dictionary.
For some locations, the acronym or abbreviation denoting a location was not in our location dictionary (e.g. \textit{Melb}).

Some location names were not in our GeoName dictionary and were neither identified by the CRF.
Examples of these location names include \textit{Coal Quay}, \textit{Pretty Bach} or \textit{Massabielle grotto}.
Some two letter US states acronyms were not recognized by our system, e.g. OR, IR, MT or MT.

In a few cases, our system missed part of the location name when it was a specific location if a location name.
For instance, \textit{markets} was not annotated in \textit{Kelvin Grove markets} or \textit{grounds} was not annotated in \textit{UTS grounds}.


\section{Conclusion and Future Work}
\label{sec:conclusion}


% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{strings,acl2014}

\end{document}
