\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath,relsize}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{enumitem}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\geoname}{\texttt{GeoNames}\xspace}
\newcommand{\mygl}[1]{``#1''}
\newcommand{\mybf}[1]{\textbf{#1}}
\newcommand{\myex}[1]{\textit{#1}}
\newcommand{\lexpair}[2]{\myex{#1}\xspace\mygl{#2}}
\newcommand{\method}[1]{\textsf{#1}\xspace}
\newcommand{\x}{\phantom{0}}
\newcommand{\ngram}{\textit{n}-gram\ }
\newcommand{\ngrams}{\textit{n}-grams\ }
\newcommand{\stanford}{\textsc{Stanford}\xspace}
\newcommand{\washington}{\textsc{Washington}\xspace}
\newcommand{\msra}{\textsc{msra}\xspace}
\newcommand{\cmu}{\textsc{cmu}\xspace}
\newcommand{\crfsuite}{\textsc{CRF-suite}\xspace}
\newcommand{\wordvec}{\textsc{Word2Vec}\xspace}
\newcommand{\feature}[1]{\texttt{#1}\xspace}
\newcommand{\myurl}[1]{{\footnotesize\url{#1}}}

\newcommand{\secref}[2][]{Section#1 \ref{#2}}
\newcommand{\secrefs}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\tabref}[2][]{Table#1 \ref{#2}}
\newcommand{\tabrefs}[2]{Tables \ref{#1} and \ref{#2}}
\newcommand{\figref}[2][]{Figure#1 \ref{#2}}
\newcommand{\eqnref}[2][]{Equation#1 \ref{#2}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% compact lists (requires enumitem)
\setlist[itemize]{noitemsep}
\setlist[enumerate]{noitemsep}

\title{Identifying Twitter Location Mentions}

\author{Bo Han, Antonio Jimeno Yepes, Andrew MacKinlay, Qiang Chen \\
  IBM Research \\
  Melbourne, VIC, Australia \\
  {\tt bohan.ibm@au1.ibm.com}, {\tt antonio.jimeno@au1.ibm.com}, \\
  {\tt admackin@au1.ibm.com}, {\tt qiangchen@au1.ibm.com} 
  \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    This paper describes our system in the ALTA shared task 2014.
    The task is to identify location mentions in Twitter messages, such as place names and point-of-interests (POIs).
    We formulated the task as a sequential labelling problem, and explored various features on top of a conditional random field (CRF) classifier.
    The system achieved 0.726 mean-F measure on the held-out evaluation data.
    We discuss our results and suggest ideas for future work on location mention recognition in social media.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The ALTA shared task 2014 aims to identify location mentions in Twitter data.
The input is plain text messages, and the expected output is location entities such as country names, city names and POIs for each message.
For instance, \myex{Auckland} and \myex{\#eqnz} are identified as location mentions in \myex{@USER are you considering an Auckland visit after \#eqnz today?}.\footnote{\myex{\#eqnz} is a short form for earthquake in New Zealand.}
This shared task is very similar to a well-established NLP task --- named entity recognition (NER) but with a focus on location entities in social media.
Each token in a text message is categorised as either a location mention or not.
The nearby tokens (\ie context) may influence a token's labelling, hence we incorporate context information in our system.
Following the literature on NER \cite{wwwc13ling}, we formulate it as a sequential labelling task and use a conditional random field (CRF) as the classifier.

The main contributions of the paper are:
(1) A sequential labeller for identifying location mentions in social media;
(2) Feature analysis and comparison in NER between social media and other genres.
(3) Discussion on errors and extensions to current sequential labeller.

\section{Challenges}
\label{sec:challenge}
Although CRF models for NER are widely used and are reported to achieve state-of-the-art results in literature \cite{acl05fink,acl11liux,emnlp11ritt}, NER in social media still raises several non-trivial challenges.

First, Twitter text is noisy, with more non-standard words than polished text \cite{baldwin2013} including typos (\eg \lexpair{challanges}{challenges}), abbreviations (\eg \lexpair{ppl}{people}) and phonetic substitutions (\eg \lexpair{4eva}{forever}).
These non-standard words often cause generalisation issues \cite{acl11han}.
For instance, lexical variants (\eg \myex{Melb}, \myex{Mel}, \myex{melbn}) will not be recognised in the test data when only standard forms (\eg \mygl{Melbourne}) are observed in the training data.

In addition to non-standard words, informal writing style further reduces NER accuracy.
One example is that conventional features relying on capitalisation are less reliable.
For instance, \myex{LOL} is capitalised but it is not a location entity, while \myex{brisbane} may be a valid location mention even though it is in lowercase.

Similarly, Twitter specific entities sometimes are sentence constituents, \eg \myex{\#Melbourne} in \myex{\#Melbourne is my fav city.}
However, they may be a topic tag that does not form part of the syntactic structure of the sentence, such as the hashtags in \myex{I like travel to beautiful places, \#travel \#melbourne}, in which case syntactic features would be less effective.

For this reason, widely-used NER features may need to be re-engineered for use over social media text.

\section{Feature Engineering}
\label{sec:feature}

\subsection{Related work for NER}
\label{sec:literature}
The starting point for our features comes from some other representative systems that are summarised in \tabref{tab:fea_comp}.

%TODO: I feel like we don't need to talk on Gibbs sampling, however, their recommended citation highlights this non-local features.
\stanford NER \cite{acl05fink} combined Gibbs sampling and a widely used CRF model.
The Gibbs sampling offers non-local constraints to the conventional CRF model that utilises a range of local features.
%TODO: the comma after \ngrams are not correct, didn't have a neat way to fix that.
The features in the CRF model are based on words, POS tags, character \ngrams, word shapes and the presence of words in a pre-defined window.
The word and POS tag features also include the surrounding tokens and tags to capture the local context information.

\newcite{acl11liux} proposed a two-stage CRF-based tagger \msra for Twitter NER.
First, a \textit{k}-NN classifier pre-categorises words, and then feeds results to a downstream CRF modeller.
The features they adopted in \textit{k}-NN are two word text windows including the target word (\ie five words in total).
The gazetted resources (from Wikipedia) are also utilised and shown to be effective in their experiments.
As for the features for building the second stage CRF model, they followed \newcite{conll09rati} and made use of tokens, word types (\eg whether the word is alphanumeric or capitalised), word morphological features (\eg suffix and prefix of words), previous tagging labels, word context windows, and conjunction features that combine both tags and word context windows.

%Among various extensions on their experiments, they also found gazetteers are quite helpful on both social media and existing datasets.
%Clustering lexical variants seems a complex story, it doesn't show clear improvement on social media dataset (as reported), but bears much improvements on the conventional datasets.

Recently, another \washington NER tool \cite{emnlp11ritt} was developed by rebuilding a Twitter-specific NLP pipeline (from tokenisation and POS tagging to chunking and NER).
They adopted rich information generated in the pipeline, such as POS tags, chunking and predicted capitalisation information, as well as clustering of lexical variants \cite{cl92brow} and gazetted features from Freebase.

% table 
\begin{table*}[!htbp]
\begin{center}
\begin{tabular}{lccc}
\toprule
Features                                     & \stanford & \msra & \washington \\ 
\midrule
\feature{Word}                               & \cmark & \cmark & \cmark \\
\feature{Word Context}                       & \cmark & \cmark & \cmark \\
\feature{Word Morphology}                    & Character \ngram  & Affix & Brown Cluster \\
%\feature{Non-local constraints}              & Gibbs sampling & \textit{k}-NN results & labelled-LDA \\
\feature{POS}                                & \cmark & \xmark & in-domain POS tagger\\
\feature{Chunking}                           & \xmark & \xmark & in-domain chunker \\
\feature{Capitalisation}                     & \xmark & \cmark & in-domain capitalisation restoration\\
\feature{Gazetteers}                         & \xmark & Wikipedia & Freebase \\
\bottomrule
\end{tabular}
\end{center}
\caption{Features comparison of representative NER Systems}
\label{tab:fea_comp}
\end{table*}


\subsection{Proposed Features}
\label{sec:basic_feature}

Based on the previous representative NER work, we considered the following features:

\begin{itemize}
    \item \feature{Word}. Lowercased word types are included as a default feature as suggested by existing systems. Previous and next two words are also included to capture local context information. Larger context window size is not considered as Twitter data is fairly terse and ungrammatical \cite{baldwin2013}, so incorporating long distance context may bring little context information and introduce more noise.
    \item \feature{POS}. Based on the fact that location named entities are primarily nouns. A reliable POS tagger generates valuable clues for locations. Instead of re-building a NLP pipeline, we adopt an off-the-shelf Twitter POS tagger \cmu that generates coursed-grained POS tags with high accuracy ($\ge$ 90\%) \cite{naacl13owop}. Similar to \feature{word}, the previous and next two \feature{POS} tags are also included. We also consider \feature{POS} bigrams.
    \item \feature{Capitalisation}. Instead of predicting token case in Twitter (\eg \cite{emnlp11ritt}), four types of capitalisation information are retrieved based on the original surface form. Namely, they are all character uppercased (AU), all character lowercased (AL), first character uppercased and the rest are lowercased (UL) and mixed capitalisation (MC). We also consider \feature{capitalisation} bigrams.
    \item \feature{Domain}. Twitter specific entities such as user mentions, hashtags and URLs are considered as normal words. This is because many location mentions are embedded in these entities. For instance, \myex{@Iran}, \myex{\#brisbane} and \myurl{http:www.abc.net.au/melbourne/}. Furthermore, we distinguish whether a word is in a stop word or not. Moreover, some location clues such as \myex{street} are also categorised as task-specific features in this feature group.
    \item \feature{Gazetteer}. Literature has shown that external gazetted resources are helpful in identifying named entities. Therefore, we incorporate features based on external place names, \eg whether a current word is in a refined list of locations. Details are in \secref{sec:gaze_feature}.
    \item \feature{Gazetteer Morphology}. As an extension of previous \feature{gazetteer} features, we also observed that gazetted names may form part of a token and this is particularly common for Twitter specific entities, \eg \myex{\#IranEQ} and \myex{@zdnetaustralia}. As a result, we also perform partial string matching in \secref{sec:mor_feature}. 
\end{itemize}

\subsection{Gazetteers}
\label{sec:gaze_feature}

We adopted \geoname as our primary source of gazetted features.
It is a geographical database with information about all countries with over eight million places, such as cities and points of interest.\footnote{\myurl{http://www.geonames.org}} 
However, as noted by \newcite{acl11liux}, some place names are also commonly used to denote something other than a location.
Examples of these terms include people's names, natural disasters (\eg \myex{storm}), and names that usually do not denote a location (\eg \myex{Friday} or \myex{Friend}).
To alleviate the negative impact of these unreliable place names, we collected stopwords starting with a standard one and then added 5K most frequent English terms,\footnote{\myurl{http://www.wordfrequency.info}} natural disaster names from Wikipedia and a list of popular personal names.\footnote{\myurl{https://online.justice.vic.gov.au/bdm/popular-names}}

After extracting and cleaning the terms from \geoname, the list had over 9.8 million terms.\footnote{Locations might have more than one name to include variants.}
The dictionary was used to annotate the tweets using ConceptMapper~\cite{tanenblatt2010conceptmapper} and the \geoname annotation was used as a CRF feature.

On top of refined gazetteers, we also collected country names, state abbreviations, airport IATA codes and place abbreviations (\eg \myex{st} for street) in some English speaking countries from Wikipedia and Google.\footnote{The data is available at \myurl{https://github.com/tq010or/alta-shared-task-2014-ibm}}
The list is also filtered by stopword removal so that it represents a high quality place names and we can separately use them as gazetted features from \geoname.

\subsection{Gazetteer Morphology}
\label{sec:mor_feature}

The unique genre in Twitter generates many composite and non-standard location mentions.
For instance, \myex{chch} represents Christchurch in New Zealand in \myex{Thoughts with everyone in chch \#eqnz - not again!}; 
A standard place name may be concatenated with other tokens, \eg \myex{\#pakistanflood}.
A naive string match will miss these gazetted terms, therefore, we also match the prefix and suffix of all refined gazetted terms in \secref{sec:gaze_feature} for each token in the tweet.
The side effect of this approach is it also produces some false positives, \eg \myex{sa} (as South Australia or South Africa) also matches \myex{samsung}.
To avoid matching false positives, we further restrict the other part (\eg \myex{msung} in the \myex{samsung} example) must be a valid word from a 81K English lexicon.\footnote{2of12inf.txt in \myurl{http://goo.gl/4c49gv}}.

Additionally, we also stripped spaces for higher order \ngram for this gazetteer morphology matching so that \myex{newzealand} and \myex{losangeles} would be recognised as well.

\section{Experiments and Discussion}
\label{sec:experiment}

% description of data and eval metrics, BIO
The training/dev/test data is offered by ALTA-shared task organisers.
The collected and filtered tweets correspond to short time period when several disastrous events happened.
2K tweets are used to training and 1K tweets are randomly selected and equally split for dev and test purpose.
The data set, however, is skewed to a number disastrous events such as New Zealand earthquake and Queensland floods.

The evaluation metric is mean-F score which averages the F1 number for each tweet.
In addition to this official evaluation metric, we also present precision, recall and F1 numbers.

% model settings: BIO, lowercase, 
We adopted a state-of-the-art CRF implementation named \crfsuite \cite{crfsuite} with default parameters.
We used built-in tokenisation and POS tags in \cmu and all our string matching is in lowercase.
Furthermore, the features are represented in \method{BIO} notation, in which \method{B} and \method{I} represent the beginning and continuity of a location entity, respectively.
\method{O} denotes non-entity words.
Using lowercased features and \method{BIO} (instead of \method{BIOLU} \cite{conll09rati}) notations are to avoid potential data sparsity issues in generalisation, as we only have 2K training tweets and many labels such as \myex{\#eqnz} are repeated fairly frequently.

% description of settings (post processing)
To correct CRF tagging errors and misses, we further imposed some post-processing rules.
Words satisfying the following rules are also added as location mentions:

\begin{enumerate}
\item A word is in the refined \geoname dictionary or a Twitter-specific entity is a gazetted term combined with an English word;
\item A word is in a closed set of direction names (\eg \myex{north}, \myex{nth} or \myex{north-east}) or location clues (\eg \myex{street}, \myex{st});
\item An URL contains an entry in the refined \geoname dictionary;
\item If the tokens preceding and following \myex{of} are labelled as locations, then the middle \myex{of} is counted as part of a location mention, \eg \myex{north \textbf{of} Brunswick Heads};
\item \myex{CFA} and \myex{CFS} with the following two words are labelled as location mentions, \eg \myex{CFA district 123}.
\end{enumerate}


%TODO: do we want to highlight the numbers?
%TODO [Antonio] Yes, what is the configuration used for our ALTA submission?
%TODO: [Bo] the configuration is based on the feature section. We incorporate all features and then examine the impact of feature abalations
\begin{table*}[!htbp]
\begin{center}
\begin{tabular}{lcccccccc}
\toprule
Data                                         & \multicolumn{4}{c}{Dev}             & \multicolumn{4}{c}{Test}        \\
\cmidrule{2-9}
Evaluation metrics                           & mean-F1 & F1    & P     & R         & mean-F1 & F1    & P     & R     \\ 
\midrule
\feature{Overall}                            & 0.758   & 0.774 & 0.784 & 0.764     & 0.726   & 0.756 & 0.770 & 0.742 \\
% [Antonio] I added this to separate the ALTA results from the other experiments
\hline
\feature{-Word}                              & 0.716   & 0.738 & 0.717 & 0.760     & 0.683   & 0.715 & 0.702 & 0.729 \\
\feature{-POS}                               & 0.744   & 0.767 & 0.780 & 0.755     & 0.713   & 0.742 & 0.772 & 0.715 \\
\feature{-Capitalisation}                    & 0.748   & 0.761 & 0.769 & 0.752     & 0.723   & 0.753 & 0.769 & 0.737 \\
\feature{-Domain}                            & 0.758   & 0.772 & 0.781 & 0.763     & 0.715   & 0.749 & 0.768 & 0.732 \\
\feature{-Gazetteer}                         & 0.751   & 0.770 & 0.776 & 0.763     & 0.725   & 0.749 & 0.758 & 0.741 \\
\feature{-Gazetteer Morph.}                  & 0.754   & 0.772 & 0.780 & 0.763     & 0.727   & 0.756 & 0.770 & 0.742 \\
\feature{-Post-processing}                   & 0.714   & 0.743 & 0.814 & 0.684     & 0.700   & 0.736 & 0.814 & 0.672 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Overall experiment results and feature ablations}
\label{tab:fea_exp}
\end{table*}

% overall accuracy and inconsistency of dev/test data
%The evaluation numbers for overall and feature ablations are presented in \tabref{tab:fea_exp}.
%Overall, our best tagger achieved 0.758 and 0.726 mean-F1 on dev and test data, respectively.
%The 3\% inconsistency suggests the evaluation data is either not on randomly partitioned or not \textit{i.i.d.}

%TODO: Antonio, could you please add some comparison with other's results here?
%TODO [Antonio] I added some comments but I find it difficult to comment since we are not aware of others' implementations and I have found some disagreements in their recently published results.
The evaluation numbers of our system for overall and feature ablations are presented in \tabref{tab:fea_exp}.
Kaggle's site shows the results on the test set of our system compared to other systems.\footnote{Challenge results on the dev set: \myurl{https://inclass.kaggle.com/c/alta-2014-challenge/leaderboard}, and public and private split of the test set: \myurl{https://inclass.kaggle.com/c/alta-2014-challenge/forums/t/10702/and-the-winner-is/57341#post57341}}
Our system seems to perform below other participating systems, which is cannot be discussed since we are not aware of the implementation of the other systems.
Overall, our best tagger achieved 0.758 and 0.726 mean-F1 on dev and test data, respectively.
The noticeable disagreements between the results on the dev and test data indicates that there is a large difference between the two sets and that larger training sets are required to avoid overfitting or that additional sets of features might be considered.

% useful features and abnormal features
Among all features, we saw that \feature{word} and \feature{post-processing} are the most important features to NER.
By contrast, \feature{domain}, \feature{gazetteer} and \feature{gazetteer morphology} contribute little to the overall performance.
It makes sense that \feature{word} are effective features, because many specific tokens (\eg \myex{eqnz}) are strong signals showing the token is a location mention.
However, it is counter-intuitive that \feature{gazetteer} and \feature{gazetteer Morphology} failed to boost the performance \cite{conll09rati,acl11liux}.
We hypothesise this may be because our CRF model down-weighted \feature{gazetteer} features, when some location mentions (such as non-gazetted POIs) are not in the refined \geoname and there might be common words that share the same surface forms with entries in \geoname.
Nonetheless, this doesn't indicate the gazetted data is not useful, but rather it should be integrated appropriately.
Because when we added gazetted data in the \feature{post-processing}, a considerable boost in performance is observed.

Notably, \feature{capitalisation} and \feature{POS} are useful in identifying location mentions.
This suggests developing reliable capitalisation restoration and NLP pipeline will be beneficial to downstream NER.

\section{Error Analysis}
\label{sec:error_analysis}
%% We could talk about the issues with our approach

Our system incorrectly identified some tokens as locations.
Most of the false positives were due to CRF mistakes.
Examples of these mistakes are annotation of tokens like \myex{bushfires}, 
Probably a larger data set would allow the CRF model to avoid these mistakes.
On the other hand, many false positives produced by our system look as genuine locations.
For instance, \myex{bakery} was not annotated in \myex{Chinatown bakery} but was annotated in \myex{Manchester Wong Wong's bakery} a few tweets below.
Some locations such as \myex{Kumbarilla State Forest} seem to be false positives as well.
Possibly the noise in the data set is also responsible for errors produced by our CRF tagger.


Even with our best efforts to remove location names that would not typically denote a location, there are some \geoname locations in our dictionary that typically do not denote a location, \eg \myex{The End of the World}.

Our system missed some Twitter user names or hashtags with location information, \eg \myex{@FireRescueNSW}, \myex{@abcsouthqld}.
Although these locations contain an acronym or abbreviation denoting a location as prefix or suffix, the rest part is not a valid single word in our English lexicon.
For some locations, their variants were not in our location dictionary, \eg \myex{Melb} for \myex{Melbourne}.

Some location names were not in our \geoname dictionary and nor were identified by the CRF.
Examples of these location names include \myex{Coal Quay} or \myex{Massabielle grotto}.
Some two letter US state abbreviations were not recognised by our system, \eg \myex{OR} or \myex{ID}; this could possibly be alleviated by less aggressively filtering the stopwords such as \myex{OR} from the gazetteer but this would in many cases result in many false positives.

In a few cases, our system missed part of the location name when it was a generic location token attached to a specific named location.
For instance, \myex{markets} was not annotated in \myex{Kelvin Grove markets} and \myex{grounds} was not annotated in \myex{UTS grounds}.

\section{Discussion}
\label{sec:discussion}

In addition to standard CRF experiments and feature ablation analysis, we also tried to improve the accuracy through two extensions.
First, we leveraged embedded topics to represent features, \ie a feature is represented by the distribution of a limited number of related topics.
The feature to topic distribution map is generated on a larger number of English tweets using \wordvec.\footnote{\myurl{https://code.google.com/p/word2vec/}}
The results, compared with the CRF experiments, turn to be negative or minor positive in various settings.
We infer this may be due to the sub-domain difference in the representation.
We used general English tweets from Twitter Streaming API to obtain the embedded topics, which is different from disaster-related tweets with location mentions.
Alternatively, this may be due to the high noise/signal ratio, \ie expanding original feature to embedded topics brings more noise than the useful information.

Additionally, we also tried semi-supervised learning by first training a CRF model to annotate locations in a large amount of unseen new tweets, then feeding all locations and tweets into a CRF learner to train a new model for future tagging.
This approach didn't show improvement either.
We hypothesise that this is due to the data set being skewed towards disaster-related location mentions, adding more training data from general tweets does not improve the results.
%We plan to explore these reasons in the future work.
 % DO WE? No we don't, so I commented the text

\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we described our system in participating ALTA-shared task --- identifying location mentions in Twitter.
We formulated the problem as a location entity recognition task to scope our efforts in NER literature.
Having examined and compared NER feature of existing systems, we proposed our own feature set with justifications.
We further built a CRF-based location mention tagger and analysed the feature contributions.
Overall, our tagger achieved 0.726 mean-F1 in the shared task.
Although our extension experiments both show negative results, there is certainly room for further improvements.
Our discussion and error analysis shed light on the future work in this research topic.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{strings,acl2014}

\end{document}
