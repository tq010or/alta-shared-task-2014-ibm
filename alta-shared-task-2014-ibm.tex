\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{amsmath,relsize}
\usepackage{pifont}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\geoname}{\texttt{GeoNames}\xspace}
\newcommand{\mygl}[1]{``#1''}
\newcommand{\mybf}[1]{\textbf{#1}}
\newcommand{\myex}[1]{\textit{#1}}
\newcommand{\lexpair}[2]{\myex{#1}\xspace\mygl{#2}}
\newcommand{\method}[1]{\textsf{#1}\xspace}
\newcommand{\x}{\phantom{0}}
\newcommand{\ngram}{\textit{n}-gram\ }
\newcommand{\stanford}{\textsc{Stanford}\xspace}
\newcommand{\washington}{\textsc{Washington}\xspace}
\newcommand{\msra}{\textsc{msra}\xspace}
\newcommand{\cmu}{\textsc{cmu}\xspace}
\newcommand{\crfsuite}{\textsc{CRF-suite}\xspace}
\newcommand{\wordvec}{\textsc{Word2Vec}\xspace}
\newcommand{\alta}{\textsc{ALTA}\xspace}
\newcommand{\feature}[1]{\texttt{#1}\xspace}
\newcommand{\myurl}[1]{{\footnotesize\url{#1}}}

\newcommand{\secref}[2][]{Section#1 \ref{#2}}
\newcommand{\secrefs}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\tabref}[2][]{Table#1 \ref{#2}}
\newcommand{\tabrefs}[2]{Tables \ref{#1} and \ref{#2}}
\newcommand{\figref}[2][]{Figure#1 \ref{#2}}
\newcommand{\eqnref}[2][]{Equation#1 \ref{#2}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\title{Identifying Twitter Location Mentions}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    This paper describes a participant system in ALTA shared task 2014.
    The task is to identify location mentions in Twitter messages, such as place names and point-of-interests (POIs).
    We formulated the task as a sequential labelling problem, and explored various features on top of a conditional random field (CRF) classifier.
    The system achieved 0.712 mean-F measure on the held-out evaluation data.
    We discussed our results and provided future work on named entity recognition in social media.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The ALTA shared task 2014 aims to identify place mentions in Twitter data.
The input is plain text messages, and the expected output is location entities such as country names, city names and POIs for each corresponding messages.
For instance, \myex{Auckland} and \myex{\#eqnz} are identified as location mentions in \myex{@USER are you considering an Auckland visit after \#eqnz today?}.\footnote{\myex{\#eqnz} is short for earthquake in New Zealand.}
This shared task is very much similar to a well-established NLP task --- named entity recognition (NER) with a focus on location entities.
For each token in a text message, it is categorised as either a location mention or not.
The nearby words (\ie context) may influence a word's labelling, hence we incorporate context information in our system.
Following the literature on NER \cite{wwwc13ling}, we formulate it as a sequential labelling task and use a conditional random field (CRF) as the classifier.

The main contributions of the paper are:
(1) A sequential labeller for identifying location mentions in social media;
(2) Feature analysis and comparison in NER between social media and other genres.
(3) Additional experiments and discussion on extending current sequential labeller.

\section{Challenges}
\label{sec:challenge}
Although CRF models for NER are widely used and are reported to achieve state-of-the-art results in literature \cite{acl05fink,acl11liux,emnlp11ritt}, NER in social media still raises several non-trivial challenges.

First, social media text is noisy.
It contains many non-standard words including typos (\eg \lexpair{challanges}{challenges}), abbreviations (\eg \lexpair{ppl}{people}) and phonetic substitutions (\eg \lexpair{4eva}{forever}).
These non-standard words often cause generalisation issue \cite{acl11han}.
For instance, lexical variants (\eg \myex{Melb}, \myex{Mel}, \myex{melbn}) won't be recognised in the test data when only standard forms (\eg \mygl{Melbourne}) are observed in the training data.

In addition to non-standard words, informal and colloquial writing style further challenges the NER accuracy.
One example is conventional features relying on capitalisation are less reliable in social media.
For instance, \myex{LOL} is capitalised but it is not a location entity.
By contrast, \myex{brisbane} is a valid location mention even though it is in lowercase.

Similarly, Twitter specific entities sometimes are sentence constituents, \eg \myex{\#Melbourne} in \myex{\#Melbourne is my fav city.}
However, they may be a topic tag that is non-compliant to the sentence syntactic structure, \eg hashtags in \myex{I like travel to beautiful places, \#travel \#melbourne}.
For the latter case, syntactic features would be less effective.

To this end, widely-used NER features require to be re-examined in social media.

\section{Feature Engineering}
\label{sec:feature}

\subsection{Brief Litearture on NER Feature Enginnering}
\label{sec:literature}
We engineer features by reviewing some representative systems.
The main features of representative systems are summarised in \tabref{tab:fea_comp}.

\stanford NER combined Gibbs sampling and a widely used CRF model \cite{acl05fink}.
The Gibbs sampling offers non-local constraints to the conventional CRF model that utilises a range of local features.
The features in the CRF model include word features, POS tag features, character \ngram features, word shape features and the presence of words in a pre-defined window.
The word and POS tag features also includes the surrounding tokens and tags to capture the local context information.

\newcite{acl11liux} proposed a two-stage CRF-based tagger \msra for Twitter NER.
First, a \textit{k}-NN classifier pre-categorises words, and then feeds results to a downstream CRF modeller.
The features they adopted in \textit{k}-NN are two word text windows including the target word (\ie five words in total).
The gazetted resources (from Wikipedia) are also utilised and shown effective in their experiments.
As for the features for building the second stage CRF model, they follow \newcite{conll09rati} and made use of words tokens, word types (\eg whether the word is alphanumeric or capitalised?), word morphological features (\eg suffix and prefix of words), previous tagging labels, word context windows, and conjunction features that combine both tags and word context windows.
%Among various extensions on their experiments, they also found gazetteers are quite helpful on both social media and existing datasets.
%Clustering lexical variants seems a complex story, it doesn't show clear improvement on social media dataset (as reported), but bears much improvements on the conventional datasets.

Recently, another \washington NER tool was developed by rebuilding a Twitter-specific NLP pipeline (\eg from tokenisation and POS tagging to chunking and NER).
They adopted rich information generated in the pipeline, such as POS tags, chunking and predicted capitalisation information, as well as Brown clustering of lexical variants and gazetted features from Freebase.

% table 
\begin{table*}[!htbp]
\begin{center}
\begin{tabular}{lccc}
\hline 
Features                                     & \stanford & \msra & \washington \\ 
\hline
\feature{Word}                               & \cmark & \cmark & \cmark \\
\feature{Word Context}                       & \cmark & \cmark & \cmark \\
\feature{Word Morphology}                    & Character \ngram  & Affix & Brown Cluster \\
%\feature{Non-local constraints}              & Gibbs sampling & \textit{k}-NN results & labelled-LDA \\
\feature{POS}                                & \cmark & \xmark & in-domain POS tagger\\
\feature{Chunking}                           & \xmark & \xmark & in-domain chunker \\
\feature{Capitalisation}                     & \xmark & \cmark & in-domain capitalisation restoration\\
\feature{Gazetteers}                         & \xmark & Wikipedia & Freebase \\
\hline
\end{tabular}
\end{center}
\caption{Features comparison of represtative NER Systems}
\label{tab:fea_comp}
\end{table*}


\subsection{Proposed Features}
\label{sec:basic_feature}

Based on the previous representative NER work, we select our features with justifications as follows:
\begin{itemize}
    \item \feature{Word}. Lowercased word types are included as a default feature as suggested by existing systems. Previous and next words are also included to capture local context information. Larger context window size is not considered as Twitter data is quite terse and ungrammatical, incorporating long distance context may bring little context information but introduce more noise;
    \item \feature{POS}. Based on the fact that location named entities are primarily nouns. A reliable POS tagger generates valuable clues for locations. Instead of re-building an NLP pipeline, we adopt an off-the-shelf Twitter POS tagger \cmu that generates coursed-grained POS tags with high accuracy ($\ge$ 90\%) \cite{naacl13owop}. Similar to \feature{Word}, the previous and next \feature{POS} tags are also included.
    \item \feature{Capitalisation}. Instead of normalising token case in Twitter (\eg \cite{emnlp11ritt}), four types of capitalisation information are retrieved based on the original surface form. Namely, they are all character uppercased (AU), all character lowercased (AL), first character uppercased and the rest are lowercased (UL) and mixed capitalisation (MC). 
    \item \feature{Domain}. Twitter specific entities such as user mentions, hashtags and URLs are considered as normal words. This is because many location mentions are embedded in these entities. For instance, \myex{@Iran}, \myex{\#brisbane} and \myurl{http:www.abc.net.au/melbourne/}.
    \item \feature{Gazetteer}. Literature has shown that external gazetted resources are helpful in identifying named entities. Therefore, we incorporate features based on external place names, \eg whether a current word is in a refined list of place names. Details are discussed in \secref{sec:gaze_feature}.
    \item \feature{Gazetteer Morphology}. As an extension of previous \feature{Gazeteer} features, we also observed that gazetted names may form part of a token and this is particularly common for Twitter specific entities, \eg \myex{\#IranEQ} and \myex{@zdnetaustralia}. As a result, we also perform partial match of string in \secref{sec:mor_feature}.
\end{itemize}

\subsection{Gazetteers}
\label{sec:gaze_feature}

We adopted \geoname as our primary source of gazetted features.
It is a geographical database with information about all countries with over eight million place names, such as city names and points of interest (POI).\footnote{GeoNames site: \myurl{http://www.geonames.org}} 
However, as noted by \newcite{acl11liux}, some place names are used as well commonly without denoting a location.
Examples of these terms include person names, natural disasters (\eg \myex{storm}), and names that usually do not denote a location (\eg \myex{Friday} or \myex{Friend}).
To alleviate the negative impact of these unreliable place names, we collected stopwords starting with a standard one and then added 5K most frequent English terms,\footnote{\myurl{http://www.wordfrequency.info}} natural disaster names from Wikipedia and a list of popular person names.\footnote{\myurl{https://online.justice.vic.gov.au/bdm/popular-names}}

After extracting and cleaning the terms from \geoname, the list has over 9.8 million terms.
The dictionary has been used to annotate the tweets using ConceptMapper~\cite{tanenblatt2010conceptmapper}.
The \geoname annotation has been used to activate CRF model which tokens might denote a location.

On top of refined gazetteers, we also collected country names, state abbreviations, airport IATA codes and place abbreviations (\eg \myex{st} for street) in some English speaking countries from Wikipedia and Google.\footnote{The statistics are available at \myurl{https://github.com/tq010or/alta-shared-task-2014-ibm}}.
The list is also filtered by stopword removing, \eg Oregon's state abbreviation \myex{OR} is removed.
This list represents a high quality place names and we separately use them as gazetted features from \geoname.

\subsection{Gazetteer Morphology}
\label{sec:mor_feature}

The unique genre in Twitter generates many composite and non-standard location mentions.
For instance, \myex{chch} represents Christan Church in New Zealand in \myex{Thoughts with everyone in chch \#eqnz - not again!}; 
A standard place name may be concatenated with other tokens, \eg \myex{\#pakistanflood}.
A naive string match will miss these gazetted terms, therefore, we also match the prefix and suffix of all refined gazetted terms in \secref{sec:gaze_feature} for each token in the tweet.
The side effect of this approach is it also produces some false positives, \eg \myex{sa} (as South Australia or South Africa) also matches \myex{samsung}.
To avoid matching false positives, we further restrict the other part (\eg \myex{msung} in the \myex{samsung} example) must be a valid word from a 81K English lexicon.\footnote{2of12inf.txt in \myurl{http://goo.gl/4c49gv}}.

Additionally, we also stripped spaces for higher order \ngram for this gazetteer morphology matching so that \myex{newzealand} and \myex{losangeles} would be recognised as well.

\section{Experiments and Discussion}
\label{sec:experiment}

% description of data and eval metrics, BIO
The training/dev/test data is offered by ALTA-shared task organisers.
The collected and filtered tweets in a short period when disastrous events happened.
2K tweets are used to training and 1K tweets are randomly selected for dev and test purpose.
The dataset, however, is skewed to a number disastrous events such as New Zealand earthquake, Queensland floods.
The evaluation metric is mean-F score which averages the F1 number for each tweet.
In addition to official evaluation metrics, we also present numbers in a confusion matrix, \ie true positive (TP), false positive (FP), true negative (TN) and false negative (FN).

% model settings: BIO, lowercase, 
We adopted a state-of-the-art CRF implementation \crfsuite \cite{crfsuite} with default parameters.
We used built-in tokenisation and POS tags in \cmu and all our string matching is in lowercase.
Furthermore, the features are represented in \method{BIO} notation, in which \method{B} and \method{I} represent the beginning and continuity of a location entity, respectively.
\method{O} denotes non-entity words.
Using lowercased features and \method{BIO} (instead of \method{BIOLU} \cite{conll09rati}) notations are to avoid potential data sparsity issues in generalisation, as we only have 2K training tweets and many labels are redundant \eg \myex{\#eqnz} is fairly frequent.

% description of settings (post processing)
To correct CRF tagging errors and misses, we further imposed some postporcessing rules to correct explicit misses made by CRF taggers.
Words satisfying the following rules are also added as location mentions:
(1) A word is in the refined \geoname or a Twitter specific entity is composited by a gazetted term and an English word;
(2) A word is in a closed set of direction names (\eg \myex{north}, \myex{nth} or \myex{north-east}) or location names (\eg \myex{street}, \myex{st});
(3) An URL contains an entry in the refined \geoname;
(4) If the preceding and previous words of \myex{of} are labelled as locations, then the middle \myex{of} is counted as part of a location mention, \eg \myex{A1058 Coast Rd};
(5) \myex{CFA} and \myex{CFS} with the following two words are labelled as location mentions, \eg \myex{CFA district 123}.

The evaluation numbers for overall and feature ablations are presented in \tabref{tab:fea_exp}.
%TODO: fill in numbers of full experiments and feature ablations
\begin{table*}[!htbp]
\begin{center}
\begin{tabular}{lccccc}
\hline 
Features                                     & mean-F1 & TP & FP & TN & FN \\ 
\hline
\feature{Overall}                            & & & & &  \\
\feature{-Word}                              & & & & &  \\
\feature{-POS}                               & & & & &  \\
\feature{-Capitalisation}                    & & & & &  \\
\feature{-Domain}                            & & & & &  \\
\feature{-Gazetteer}                         & & & & &  \\
\feature{-Gazetteer Morphology}              & & & & &  \\
\feature{-Postprocessing}                    & & & & &  \\
\hline
\end{tabular}
\end{center}
\caption{Overall experiment results and feature ablations}
\label{tab:fea_exp}
\end{table*}

%TODO: do feature contribution analysis, I need to check the results for the discussion.
Among all features, we saw that ...

\section{Error Analysis}
\label{sec:error_analysis}
%% We could talk about the issues with our approach

Our system incorrectly identified some tokens as locations.
Most of the false positives were due to CRF mistakes.
Examples of these mistakes are annotation of tokens like \myex{bushfires}, 
Probably a larger data set would allow the CRF model to avoid these mistakes.
On the other hand, many false positives produced by our system look as genuine locations.
For instance, \myex{bakery} was not annotated in \myex{Chinatown bakery} but is was annotated in \myex{Manchester Wong Wong's bakery} a few tweets below.
Some locations such as \myex{Kumbarilla State Forest} or \myex{Shire of Carnar} seem to be false positives as well.
Possibly the noise in the data set is responsible as well for errors produced by our CRF tagger.

Even with our best efforts to remove location names that would not typically denote a location, there are some GeoNames locations in our dictionary that typically do not denote a location, e.g.~\myex{the end of the world}.

Our system missed some Twitter user names or hashtags with location information (e.g. \myex{@FireRescueNSW}, \myex{@abcsouthqld}, \myex{\#NZquake})
These locations contain an acronym or abbreviation denoting a location as prefix or suffix of a word that was not in our dictionary.
For some locations, the acronym or abbreviation denoting a location was not in our location dictionary (e.g. \myex{Melb}).

Some location names were not in our \geoname dictionary and were neither identified by the CRF.
Examples of these location names include \myex{Coal Quay}, \myex{Pretty Bach} or \myex{Massabielle grotto}.
Some two letter US states acronyms were not recognized by our system, e.g. OR, IR, MT or MT.

In a few cases, our system missed part of the location name when it was a specific location if a location name.
For instance, \myex{markets} was not annotated in \myex{Kelvin Grove markets} or \myex{grounds} was not annotated in \myex{UTS grounds}.

\section{Discussion}
\label{sec:discussion}

In addition to standard CRF experiments and feature ablation analysis, we also tried to improve the accuracy from two extensions.
First, we leveraged embedded topics to represent features, \ie a feature is represented by the distribution of a limited number of related topics.
The feature to topic distribution map is generated on a larger number of English tweets using \wordvec.
The results, compared with the CRF experiments, turn to be negative or minor positive in various settings.
We infer this may be due to the sub-domain difference in the representation.
We used general English tweets from Twitter Streaming API to obtain the embedded topics, this is different from disaster-related tweets with location mentions.
Alternatively, this may be due to the high noise/signal ratio, \ie expanding original feature to embedded topics brings more noise than the useful information.

Additionally, we also tried semi-supervised learning by first train a CRF model to annotate locations in a large amount of unseen new tweets, then all locations and tweets are fed into CRF to train a new CRF model for future tagging.
This approach didn't show improvement either.
We hypotheses this is due to the same reason that the dataset is skewed to disaster-related location mentions and using more training data from general tweets do not improve the results.
We plan to explore these reasons in the future work.


\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we described our system in participating ALTA-shared 2014 task --- identifying location mentions in Twitter.
We formulated the problem as a location entity recognition task to scope our efforts in NER literature.
Having examined and compared NER feature of existing systems, we proposed our own feature set with justifications.
We further built a CRF-based location mention tagger and analysed the feature contributions.
Overall, our tagger achieved 0.712 mean-F1 in the shared task.
Although our extension experiments both show negative results, there is certainly room for further improvements.
Our discussion and error analysis sheds lights on the future work in this research topic

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{strings,acl2014}

\end{document}
